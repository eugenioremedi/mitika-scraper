name: Daily Scrape and Upload

on:
  schedule:
    # Runs at 08:00 UTC every day
    - cron: "0 8 * * *"
  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2 playwright pandas openpyxl
          playwright install chromium

      - name: Run Scraper Script
        run: python scraper.py
        env:
          # Pass any environment variables your scraper needs
          MITIKA_USERNAME: ${{ secrets.MITIKA_USERNAME }}
          MITIKA_PASSWORD: ${{ secrets.MITIKA_PASSWORD }}

      - name: Upload to Google Drive
        # Identify the file to upload.
        # The scraper generates files in 'output/BOOKINGS_yyyy_mm_dd.xlsx'
        run: |
          # Find the generated file in the output directory
          FILE_TO_UPLOAD=$(find output -name "BOOKINGS_*.xlsx" | head -n 1)

          if [ -z "$FILE_TO_UPLOAD" ]; then
            echo "No file found to upload!"
            exit 1
          fi

          echo "Found file: $FILE_TO_UPLOAD"

          # Run the upload script
          python upload_to_drive.py "$FILE_TO_UPLOAD" "${{ secrets.GDRIVE_FOLDER_ID }}"
        env:
          GDRIVE_SERVICE_ACCOUNT_KEY: ${{ secrets.GDRIVE_SERVICE_ACCOUNT_KEY }}
